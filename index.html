<!DOCTYPE html>

<html>

<head lang="en">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>TH-PAD</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
    <link rel="stylesheet" href="./files/codemirror.min.css">
    <link rel="stylesheet" href="./files/app.css">
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-20 text-center">
                <br></br>
                <b>TH-PAD</b>: Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors<br>
<!--                 <small>
                    CVPR 2022 (Oral Presentation)
                </small> -->
            </h1>
            <hr style="margin-top:0px">
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a style="font-size: 16px;">
                            Zhentao Yu
                        </a>
                        <!-- <sup>1</sup> -->
                    </li>
                    <li>
                        <a href="https://zxyin.github.io" style="font-size: 16px;">
                            Zixin Yin
                        </a>
                        <!-- <sup>1,2</sup> -->
                    </li>
                    <li>
                        <a style="font-size: 16px;">
                            Deyu Zhou
                        </a>
                        <!-- <sup>1,2</sup> -->
                    </li>
                    <li>
                        <a href="https://dorniwang.github.io" style="font-size: 16px;">
                            Duomin Wang
                        </a>
                        <!-- <sup>1,2</sup> -->
                    </li>
                    <li>
                        <a style="font-size: 16px;">
                            Finn Wong
                        </a>
                        <!-- <sup>1,2</sup> -->
                    </li>
                    <li>
                        <a href="https://sites.google.com/site/zjuwby/?pli=1" style="font-size: 16px;">
                            Baoyuan Wang
                        </a>
                        <!-- <sup>1</sup> -->
                    </li><br>
                    <a></a><br>
                    <li>
                        <!-- <sup>1</sup> -->
                        <a href="https://www.xiaoice.com/" style="font-size: 16px;">
                            Xiaobing.ai
                        </a>
                    </li>
                    <!-- <li>
                        <sup>2</sup>
                        <a href="https://hkust.edu.hk/?cn=1"
                            style="font-size: 16px;">
                            The Hong Kong University of Science and Technology
                        </a>
                    </li> -->
<!--                     <li>
                        <sup>3</sup>
                        <a href="http://en.ustc.edu.cn/" style="font-size: 16px;">
                            University of Science and Technology of China
                        </a>
                    </li> -->
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2212.04248">
                            <img src="./files/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a>
                            <img src="./files/github.png" height="60px">
                            <h4><strong>Code (coming soon)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <a>
                    <img src="./files/teaser.png" class="img-responsive" alt="teaser"><br>
                </a>
                <p class="text-justify" style="font-size: 16px;">
                    Given only an audio source and an arbitrary identity image, our system can generate a video with natural-looking and diverse facial
                    motions (pose, expression, blink & gaze), while maintaining accurate audio-lip synchronization. Here we show randomly sampled sequences
                    from our diffusion prior for two identities, note that the lip-irrelevant facial motion varies but the lip is still in-sync.
                </p>
                <br></br>
                <h2>
                    Abstract
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    In this paper, we introduce a simple and novel framework for one-shot audio-driven talking head generation. Unlike prior works that
                    require additional driving sources for controlled synthesis in a deterministic manner, we instead probabilistically sample all the holistic
                    lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining
                    both the photo-realism of audio-lip synchronization and the overall naturalness. This is achieved by our newly proposed audio-to-visual
                    diffusion prior trained on top of the mapping between audio and disentangled non-lip facial representations. Thanks to the probabilistic
                    nature of the diffusion prior, one big advantage of our framework is it can synthesize diverse facial motion sequences given the same audio
                    clip, which is quite user-friendly for many real applications. Through comprehensive evaluations on public benchmarks, we conclude that
                    (1) our diffusion prior outperforms auto-regressive prior significantly on almost all the concerned metrics; (2) our overall system is
                    competitive with prior works in terms of audio-lip synchronization but can effectively sample rich and natural-looking lip-irrelevant
                    facial motions while still semantically harmonized with the audio input.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Video
                </h2>
                <hr style="margin-top:0px">
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/t4NhpweqfL4" allowfullscreen=""
                            style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Overview
                </h2>
                <hr style="margin-top:0px">
                <img src="./files/overview.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify" style="font-size: 16px;">
                    The overall pipeline of our proposed framework. The dotted lines represent the loss functions that are used only in training, i.e.,
                    L_{cl}, L_{ol} and L_{a2nl}. L_{G} is the training loss for visual non-lip space. Note that there is a switch in the figure, which means
                    the different forward processes in training and inference: in training, the concatenated feature f_{cat} = cat(f_{id}, f_{nl}^{v},
                    f_{l}^a); at inference stage, f_{cat} = cat(f_{id}, f_{nl}^a, f_{l}^a). Thus the part of Lip & Non-lip Disentanglement is not needed
                    anymore at the inference stage.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Generation Results
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    TH-PAD is able to produce audio-drivin talking head videos for different identities. Unlike most other methods that require additional driving
                    signals for non-lip motions, it can sample reasonable signals from the audio source.
                </p>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/generation_results.mp4" type="video/mp4">
                </video>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    TH-PAD can also generate with audio sources in different languages.
                </p>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/different_language.mp4" type="video/mp4">
                </video>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Compared with other methods, TH-PAD is able to generate richer facial motions with more accurate lip motion. Moreover, it can bring about different
                    results for the same audio source.
                </p>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/comparison1.mp4" type="video/mp4">
                </video>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/comparison2.mp4" type="video/mp4">
                </video>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/comparison3.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Ablation Study
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    Without velocity loss L_{vel}, the prediction becomes unstable. While training without the editing mechanism, we applied editing only during sampling
                    as in MDM, and observed that it jitters between adjacent frames.
                </p>
                <video style="width:100%;height:100%;" controls style="margin: 5px;" >
                    <source src="./files/ablation.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0">
                <div class="text-center">
                    <h2>
                        Citation
                    </h2>
                </div>
                <hr style="margin-top:0px">
                <div class="form-group col-md-12 col-md-offset-0">
                    <div class="CodeMirror cm-s-default CodeMirror-wrap" style="font-size: 16px;">
                        <div
                            style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 4px; left: 4px; ">
                            <textarea autocorrect="off" autocapitalize="off" spellcheck="false"
                                style="position: absolute; padding: 0px; width: 1000px; height: 1em; outline: none;"
                                tabindex="0"></textarea></div>
                        <div class="CodeMirror-vscrollbar" cm-not-content="true">
                            <div style="min-width: 1px; height: 0px;"></div>
                        </div>
                        <div class="CodeMirror-hscrollbar" cm-not-content="true">
                            <div style="height: 100%; min-height: 1px; width: 0px;"></div>
                        </div>
                        <div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-gutter-filler" cm-not-content="true"></div>
                        <div class="CodeMirror-scroll" tabindex="-1">
                            <div class="CodeMirror-sizer"
                                style="margin-left: 0px; margin-bottom: -17px; border-right-width: 13px; min-height: 162px; padding-right: 0px; padding-bottom: 0px;">
                                <div style="position: relative; top: 0px;">
                                    <div class="CodeMirror-lines">
                                        <div style="position: relative; outline: none;">
                                            <div class="CodeMirror-measure">AخA</div>
                                            <div class="CodeMirror-measure"></div>
                                            <div style="position: relative; z-index: 1;"></div>
                                            <div class="CodeMirror-cursors">
                                            <div class="CodeMirror-cursor"
                                                style="left: 4px; top: 0px; height: 17.1406px;">&nbsp;</div>
                                            </div>
                                            <div class="CodeMirror-code" style="">
                                            <pre class=" CodeMirror-line "><span style="padding-right: 0.1px;"></span></pre>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div style="position: absolute; height: 13px; width: 1px; top: 280px;"></div>
                            <div class="CodeMirror-gutters" style="display: none; height: 300px;"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 col-md-offset-0 text-center">
                <br></br>
                <h2>
                    Acknowledgements
                </h2>
                <hr style="margin-top:0px">
                <p class="text-justify" style="font-size: 16px;">
                    <!-- We thank Harry Shum for the fruitful advice and discussion to improve the paper. <br> -->
                    The website template was adapted from <a href="https://dorniwang.github.io/PD-FGC/">PD-FGC</a>.
                </p>
            </div>
        </div>
</body>
</html>
